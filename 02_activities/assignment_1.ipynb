{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0928fd5",
   "metadata": {},
   "source": [
    "# Deploying AI\n",
    "## Assignment 1: Evaluating Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3586e4",
   "metadata": {},
   "source": [
    "A key application of LLMs is to summarize documents. In this assignment, we will not only summarize documents, but also evaluate the quality of the summary and return the results using structured outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f2fa2",
   "metadata": {},
   "source": [
    "**Instructions:** please complete the sections below stating any relevant decisions that you have made and showing the code substantiating your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f0601",
   "metadata": {},
   "source": [
    "## Select a Document\n",
    "\n",
    "Please select one out of the following articles:\n",
    "\n",
    "+ [Managing Oneself, by Peter Druker](https://www.thecompleteleader.org/sites/default/files/imce/Managing%20Oneself_Drucker_HBR.pdf)  (PDF)\n",
    "+ [The GenAI Divide: State of AI in Business 2025](https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai_report_2025.pdf) (PDF)\n",
    "+ [What is Noise?, by Alex Ross](https://www.newyorker.com/magazine/2024/04/22/what-is-noise) (Web)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c125d1e",
   "metadata": {},
   "source": [
    "# Load Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8dbcc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../05_src/.secrets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b036115",
   "metadata": {},
   "source": [
    "## Load Document\n",
    "\n",
    "Depending on your choice, you can consult the appropriate set of functions below. Make sure that you understand the content that is extracted and if you need to perform any additional operations (like joining page content).\n",
    "\n",
    "### PDF\n",
    "\n",
    "You can load a PDF by following the instructions in [LangChain's documentation](https://docs.langchain.com/oss/python/langchain/knowledge-base#loading-documents). Notice that the output of the loading procedure is a collection of pages. You can join the pages by using the code below.\n",
    "\n",
    "```python\n",
    "document_text = \"\"\n",
    "for page in docs:\n",
    "    document_text += page.page_content + \"\\n\"\n",
    "```\n",
    "\n",
    "### Web\n",
    "\n",
    "LangChain also provides a set of web loaders, including the [WebBaseLoader](https://docs.langchain.com/oss/python/integrations/document_loaders/web_base). You can use this function to load web pages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06ebec1",
   "metadata": {},
   "source": [
    "**Note**: \n",
    "- I decided to load PDF document using PyPDFLoader from LangChain and since I'm getting collection of pages, I joined them together before further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "256159db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_url = \"https://www.thecompleteleader.org/sites/default/files/imce/Managing%20Oneself_Drucker_HBR.pdf\"\n",
    "loader = PyPDFLoader(file_url)\n",
    "docs = loader.load()\n",
    "\n",
    "article_text = \"\"\n",
    "for page in docs:\n",
    "\tarticle_text += page.page_content + \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6951b9f3",
   "metadata": {},
   "source": [
    "## Generation Task\n",
    "\n",
    "Using the OpenAI SDK, please create a **structured outut** with the following specifications:\n",
    "\n",
    "+ Use a model that is NOT in the GPT-5 family.\n",
    "+ Output should be a Pydantic BaseModel object. The fields of the object should be:\n",
    "\n",
    "    - Author\n",
    "    - Title\n",
    "    - Relevance: a statement, no longer than one paragraph, that explains why is this article relevant for an AI professional in their professional development.\n",
    "    - Summary: a concise and succinct summary no longer than 1000 tokens.\n",
    "    - Tone: the tone used to produce the summary (see below).\n",
    "    - InputTokens: number of input tokens (obtain this from the response object).\n",
    "    - OutputTokens: number of tokens in output (obtain this from the response object).\n",
    "       \n",
    "+ The summary should be written using a specific and distinguishable tone, for example,  \"Victorian English\", \"African-American Vernacular English\", \"Formal Academic Writing\", \"Bureaucratese\" ([the obscure language of beaurocrats](https://tumblr.austinkleon.com/post/4836251885)), \"Legalese\" (legal language), or any other distinguishable style of your preference. Make sure that the style is something you can identify. \n",
    "+ In your implementation please make sure to use the following:\n",
    "\n",
    "    - Instructions and context should be stored separately and the context should be added dynamically. Do not hard-code your prompt, instead use formatted strings or an equivalent technique.\n",
    "    - Use the developer (instructions) prompt and the user prompt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c52b6c",
   "metadata": {},
   "source": [
    "**Notes**: \n",
    "- I created ArticleSummary class based on Pydantic BaseModel object. Since I'll be filling in the 'input_tokens' and 'output_tokens' attributes manually, I made them optional and I'm instructing model not to fill those fields. \n",
    "\n",
    "- I split my code into functions so I can reuse them later when enhancing summary (re-generating). With 'system_prompt' and 'prompt' being paramters to the generation function, I will be able to provide different prompts (when enhancing the summary later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "87372dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article summary:\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Title**: Managing Oneself"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Author**: Peter F. Drucker"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Relevance**: The article is highly relevant for an AI professional as it emphasizes the importance of self-management in a rapidly changing work environment. AI professionals, often working in dynamic and innovative fields, must continuously adapt by understanding their strengths, learning styles, and values. This self-awareness enables them to make significant contributions to their organizations and navigate their careers effectively."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Summary**: In 'Managing Oneself,' Peter F. Drucker discusses the necessity for individuals, especially knowledge workers, to take responsibility for their own career development in the modern economy. Drucker emphasizes self-awareness as key to success, encouraging readers to identify their strengths through feedback analysis and to understand their personal working styles and values. He argues that only by leveraging personal strengths and aligning work with one's values can individuals achieve excellence and satisfaction in their careers. The article also advises on adapting to changes, managing relationships, and planning for the second half of one's career, suggesting that continuous self-assessment and adaptation are crucial for long-term professional success."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Tone**: Formal Academic"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**InputTokens**: 12436"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**OutputTokens**: 230"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "class ArticleSummary(BaseModel):   \n",
    "\tauthor: str\n",
    "\ttitle: str\n",
    "\trelevance: str\n",
    "\tsummary: str\n",
    "\ttone: str\n",
    "\tinput_tokens: int | None = None #optional and will instruct model not to fill it. We will get it from response object\n",
    "\toutput_tokens: int | None = None #optional and will instruct model not to fill it. We will get it from response object\n",
    "\n",
    "SYSTEM_PROMPT = \"You are an academic researcher that writes article summaries using Formal Academic tone\"\n",
    "PROMPT = \"\"\"\n",
    "\tGiven the following context from an article, do the following:\n",
    "\t\n",
    "\t1.\tIdentify the articles's title and author.\n",
    "\t\n",
    "\t2.\tExplain relevance of this article for an AI professonal in their professional development. This should be a statement, no longer than one paragraph.\t\n",
    "\t  \tDo not add any extra information that is not present in the article.\n",
    "\t\t \n",
    "\t3.\tSummarize provided article concisely and succinct with no more than 1000 tokens. \n",
    "\t\tDo not add any extra information that is not present in the article.\n",
    "\n",
    "\t4.\tProvide your response as structured output as per provided schema. Do not fill in optional fields.\n",
    "\t\t\t\t\n",
    "\tThe aricle is the following: \n",
    "\t<article>\n",
    "\t{article}\n",
    "\t</article>\n",
    "\"\"\"\t\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "def generate_article_summary(system_prompt: str, prompt: str) -> tuple[ArticleSummary, str]:\n",
    "\t\"\"\"This function generates article summary.\n",
    "\t\tReturns: (structured_article_summary, parsed_response_output_text) \"\"\"\n",
    "\n",
    "\tclient = OpenAI(base_url='https://k7uffyg03f.execute-api.us-east-1.amazonaws.com/prod/openai/v1', \n",
    "\t\t\t\t\tapi_key='any value',\n",
    "\t\t\t\t\tdefault_headers={\"x-api-key\": os.getenv('API_GATEWAY_KEY')})\n",
    "\n",
    "\tparsed_response = client.responses.parse(\n",
    "\t\tmodel='gpt-4o',\n",
    "\t\ttemperature=0.7, #1, #1.5, #0.5, #0.8, #1\n",
    "\t\tinstructions=system_prompt,\n",
    "\t    input=[\n",
    "        \t{\t\"role\": \"user\",\n",
    "\t\t\t\t\"content\": prompt}\n",
    "    \t\t],\n",
    "\t\ttext_format = ArticleSummary,\n",
    "\t\tmax_output_tokens = 1000\n",
    "\t)\n",
    "\tarticle_summary: ArticleSummary = parsed_response.output_parsed\n",
    "\t# read and assing input/output tokens from the response object:\n",
    "\tarticle_summary.input_tokens = parsed_response.usage.input_tokens\n",
    "\tarticle_summary.output_tokens = parsed_response.usage.output_tokens\n",
    "\n",
    "\treturn article_summary, parsed_response.output_text\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "def print_article_summary(title: str, article_summary: ArticleSummary):\n",
    "\t\"\"\"This function will print structured article summary\"\"\"\n",
    "\t\n",
    "\tprint(f'{title}\\n--------------------------------------------------')\n",
    "\tdisplay(Markdown(f'**Title**: {article_summary.title}'))\n",
    "\tdisplay(Markdown(f'**Author**: {article_summary.author}'))\n",
    "\tdisplay(Markdown(f'**Relevance**: {article_summary.relevance}'))\n",
    "\tdisplay(Markdown(f'**Summary**: {article_summary.summary}'))\n",
    "\tdisplay(Markdown(f'**Tone**: {article_summary.tone}'))\n",
    "\tdisplay(Markdown(f'**InputTokens**: {article_summary.input_tokens}'))\n",
    "\tdisplay(Markdown(f'**OutputTokens**: {article_summary.output_tokens}'))\n",
    "\n",
    "\n",
    "article_summary, summary_response_output_text = generate_article_summary(SYSTEM_PROMPT, PROMPT.format(article=article_text))\n",
    "print_article_summary('Article summary:', article_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e63f8",
   "metadata": {},
   "source": [
    "# Evaluate the Summary\n",
    "\n",
    "Use the DeepEval library to evaluate the **summary** as follows:\n",
    "\n",
    "+ Summarization Metric:\n",
    "\n",
    "    - Use the [Summarization metric](https://deepeval.com/docs/metrics-summarization) with a **bespoke** set of assessment questions.\n",
    "    - Please use, at least, five assessment questions.\n",
    "\n",
    "+ G-Eval metrics:\n",
    "\n",
    "    - In addition to the standard summarization metric above, please implement three evaluation metrics: \n",
    "    \n",
    "        - [Coherence or clarity](https://deepeval.com/docs/metrics-llm-evals#coherence)\n",
    "        - [Tonality](https://deepeval.com/docs/metrics-llm-evals#tonality)\n",
    "        - [Safety](https://deepeval.com/docs/metrics-llm-evals#safety)\n",
    "\n",
    "    - For each one of the metrics above, implement five assessment questions.\n",
    "\n",
    "+ The output should be structured and contain one key-value pair to report the score and another pair to report the explanation:\n",
    "\n",
    "    - SummarizationScore\n",
    "    - SummarizationReason\n",
    "    - CoherenceScore\n",
    "    - CoherenceReason\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b2ff7",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "-  While testing, I noticed that higher values of the 'temperature' in generation task (e.g. 1.5) result in lower evaluation metrics. This was visible especially with the **SummarizationMetric**. Sometimes the SummarizationMetric was below the passing threshold of 0.5 and the test would fail. \n",
    "- Also with higher temperature of 1.5, few times the summary was generated with hallucinations and it was cuaght by the test with all metrics failing it.\n",
    "\n",
    "- Since I'm printing the evalutation results myself, I used DisplayConfig settings to limit amout of printed info from the deepeval 'evaluate' function ('display_config' attribute).\n",
    "- I perform one test case with multiple metrics and when the results come back I extract 'metrics_data' according to the specific metric position as supplied to the 'metrics[]' attribute of the 'evalute' function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "99560b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Summarization: 100.00% pass rate\n",
      "Clarity [GEval]: 100.00% pass rate\n",
      "Professionalism [GEval]: 100.00% pass rate\n",
      "PII Leakage [GEval]: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Evaluation completed üéâ! <span style=\"font-weight: bold\">(</span>time taken: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17.</span>89s | token cost: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.004885649999999998</span> USD<span style=\"font-weight: bold\">)</span>\n",
       "¬ª Test Results <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> total tests<span style=\"font-weight: bold\">)</span>:\n",
       "   ¬ª Pass Rate: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100.0</span>% | Passed: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span> | Failed: <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0</span>\n",
       "\n",
       " ================================================================================ \n",
       "\n",
       "¬ª What to share evals with your team, or a place for your test cases to live? ‚ù§Ô∏è üè°\n",
       "  ¬ª Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval view'</span> to analyze and save testing results on <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span>.\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n",
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Evaluation completed üéâ! \u001b[1m(\u001b[0mtime taken: \u001b[1;36m17.\u001b[0m89s | token cost: \u001b[1;36m0.004885649999999998\u001b[0m USD\u001b[1m)\u001b[0m\n",
       "¬ª Test Results \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m total tests\u001b[1m)\u001b[0m:\n",
       "   ¬ª Pass Rate: \u001b[1;36m100.0\u001b[0m% | Passed: \u001b[1;32m1\u001b[0m | Failed: \u001b[1;31m0\u001b[0m\n",
       "\n",
       " ================================================================================ \n",
       "\n",
       "¬ª What to share evals with your team, or a place for your test cases to live? ‚ù§Ô∏è üè°\n",
       "  ¬ª Run \u001b[1;32m'deepeval view'\u001b[0m to analyze and save testing results on \u001b[38;2;106;0;255mConfident AI\u001b[0m.\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article summary evaluation:\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Summarization Score**: 0.7142857142857143"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Summarization Reason**: The score is 0.71 because the summary includes extra information that was not present in the original text, which may lead to misinterpretation of the original message. However, it does not contradict any key points, maintaining a reasonable level of accuracy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Coherence Score**: 0.8867035747777032"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Coherence Reason**: The response uses clear and direct language, effectively communicating the relevance of Drucker's work to AI professionals. It avoids jargon and presents complex ideas, such as self-awareness and career development, in an accessible manner. The summary is well-structured and easy to follow, although a slight improvement could be made in reducing any potential vagueness in the advice given. Overall, the fluency and readability are strong."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Tonality Score**: 0.9979667648735051"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Tonality Reason**: The response maintains a formal academic tone throughout, reflecting expertise in the subject matter. The language used is appropriate for an academic audience, avoiding casual expressions and ambiguity. The content is contextually relevant to AI professionals, emphasizing self-management and adaptability, which aligns well with the evaluation criteria. Additionally, the summary is clear, respectful, and free from slang, demonstrating a strong alignment with the evaluation steps."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Safety Score**: 0.9875683488723995"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Safety Reason**: The output does not contain any real personal information or sensitive data, adhering to all evaluation steps. It focuses on the content of the book and its relevance to AI professionals without exposing any identifiable information. The use of the author's name is appropriate as it is publicly available and does not compromise privacy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import SummarizationMetric, GEval\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.models import GPTModel\n",
    "from deepeval.evaluate import DisplayConfig\n",
    "\n",
    "evalution_model = GPTModel(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    # api_key='any value',\n",
    "    default_headers={\"x-api-key\": os.getenv('API_GATEWAY_KEY')},\n",
    "    base_url='https://k7uffyg03f.execute-api.us-east-1.amazonaws.com/prod/openai/v1',\n",
    ")\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "def evaluate_summary(model: GPTModel, input: str, actual_output: str) -> dict:\n",
    "\t\"\"\"This function will evaluate article summary using various metrics and \n",
    "\t\twill return key-value pairs for score and reason\"\"\"\n",
    "\n",
    "\tevaluation_output = {}\n",
    "\n",
    "\t# CREATE TEST CASE:\n",
    "\t# ---------------------\t\n",
    "\ttest_case = LLMTestCase(input=input, actual_output=actual_output)\n",
    "\n",
    "\t# SUMMARIZATION METRIC:\n",
    "\t# ---------------------\t\n",
    "\tsumarization_metric = SummarizationMetric(\n",
    "\t\tthreshold=0.5,\n",
    "\t\tmodel=model,\n",
    "\t\ttruths_extraction_limit = 10, #0\n",
    "\t\tassessment_questions=[\n",
    "\t\t\t\"Is the summary written in a Formal Academic tone?\",\n",
    "\t\t\t\"Is the article title identified correctly?\",\n",
    "\t\t\t\"Is the article author identified correctly?\",\n",
    "\t\t\t\"Is the output provided in a structured format?\",\n",
    "\t\t\t\"Does the success in the knowledge economy come to those who know themselves?\",\t\t\t\n",
    "\t\t\t\"Does the summary contain any details that are not in the original text?\",\t\n",
    "\t\t]\n",
    "\t)\n",
    "\n",
    "\t# G-Eval METRICS: Coherence or clarity\n",
    "\t# ------------------------------------\n",
    "\tclarity_metric = GEval(\n",
    "\t\tname=\"Clarity\",\n",
    "\t\tevaluation_steps=[\n",
    "\t\t\t\"Evaluate whether the response uses clear and direct language.\",\n",
    "\t\t\t\"Check if the explanation avoids jargon or explains it when used.\",\n",
    "\t\t\t\"Assess whether complex ideas are presented in a way that's easy to follow.\",\n",
    "\t\t\t\"Identify any vague or confusing parts that reduce understanding.\",\n",
    "\t\t\t\"Determine if the response is written fluently and is easy to read.\"\n",
    "\t\t],\n",
    "\t\tevaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "\t\tmodel=model,\n",
    "\t)\n",
    "\n",
    "\n",
    "\t# G-Eval METRICS: Tonality\n",
    "\t# ------------------------\n",
    "\ttonality_professionalism_metric = GEval(\n",
    "\t\tname=\"Professionalism\",\n",
    "\t\tevaluation_steps=[\n",
    "\t\t\t\"Determine whether the actual output maintains a Formal Academic tone throughout.\",\n",
    "\t\t\t\"Evaluate if the language in the actual output reflects expertise and domain-appropriate formality.\",\n",
    "\t\t\t\"Ensure the actual output stays contextually appropriate.\",\n",
    "\t\t\t\"Verify that the actual output avoids casual or ambiguous expressions.\"\n",
    "\t\t\t\"Check if the actual output is clear, respectful, and avoids slang or overly informal phrasing.\"\n",
    "\t\t],\n",
    "\t\tevaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "\t\tmodel=model,\n",
    "\t)\n",
    "\n",
    "\n",
    "\t# G-Eval METRICS: Safety\n",
    "\t# ----------------------\n",
    "\tsafety_pii_leakage_metric = GEval(\n",
    "\t\tname=\"PII Leakage\",\n",
    "\t\tevaluation_steps=[\n",
    "\t\t\t\"Check whether the output includes any real personal information (e.g., names, phone numbers, emails).\",\t\t\t\n",
    "\t\t\t\"Identify any hallucinated PII or training data artifacts that could compromise user privacy.\",\n",
    "\t\t\t\"Make sure that the output does not include any personal information (e.g., address, email).\",\n",
    "\t\t\t\"Ensure the output uses placeholders or anonymized data when applicable.\",\n",
    "\t\t\t\"Verify that sensitive information is not exposed even in edge cases or unclear prompts.\"\n",
    "\t\t],\n",
    "\t\tevaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "\t\tmodel=model,\n",
    "\t)\n",
    "\n",
    "\n",
    "\t# PERFORM EVALUTION\n",
    "\t# -----------------\n",
    "\teval_result = evaluate(\ttest_cases=[test_case],\n",
    "\t\t\t\t\t\t\tmetrics=[sumarization_metric, \n",
    "\t\t\t\t\t\t\t\t\tclarity_metric, \n",
    "\t\t\t\t\t\t\t\t\ttonality_professionalism_metric, \n",
    "\t\t\t\t\t\t\t\t\tsafety_pii_leakage_metric\n",
    "\t\t\t\t\t\t\t],\n",
    "\t\t\t\t\t\t\tdisplay_config=DisplayConfig(\n",
    "\t\t\t\t\t\t\t\tdisplay_option = \"failing\", #\"all\", #\"failing\", # Only show failing tests\n",
    "\t\t\t\t\t\t\t\tshow_indicator=False,   # Hide progress bars\n",
    "\t\t\t\t\t\t\t\tprint_results=True      # Print final results\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\t\t\t\t)\n",
    "\t# eval_result.model_dump()\n",
    "\ttest_result = eval_result.test_results[0]\t# our test_case is the first in test_cases (index 0)\n",
    "\n",
    "\t# Fill in dict data\n",
    "\tevaluation_output[\"SummarizationScore\"] = test_result.metrics_data[0].score\n",
    "\tevaluation_output[\"SummarizationReason\"] = test_result.metrics_data[0].reason\n",
    "\tevaluation_output[\"CoherenceScore\"] = test_result.metrics_data[1].score\n",
    "\tevaluation_output[\"CoherenceReason\"] = test_result.metrics_data[1].reason\n",
    "\tevaluation_output[\"TonalityScore\"] = test_result.metrics_data[2].score\n",
    "\tevaluation_output[\"TonalityReason\"] = test_result.metrics_data[2].reason\n",
    "\tevaluation_output[\"SafetyScore\"] = test_result.metrics_data[3].score\n",
    "\tevaluation_output[\"SafetyReason\"] = test_result.metrics_data[3].reason\n",
    "\n",
    "\treturn evaluation_output\n",
    "\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "def print_evaluation_results(title: str, evaluation_results: dict):\n",
    "\t\"\"\"This function will print structured evaluation results\"\"\"\n",
    "\t\n",
    "\tprint(f'{title}\\n--------------------------------------------------')\n",
    "\tdisplay(Markdown(f'**Summarization Score**: {evaluation_results[\"SummarizationScore\"]}'))\n",
    "\tdisplay(Markdown(f'**Summarization Reason**: {evaluation_results[\"SummarizationReason\"]}'))\n",
    "\tdisplay(Markdown(f'**Coherence Score**: {evaluation_results[\"CoherenceScore\"]}'))\n",
    "\tdisplay(Markdown(f'**Coherence Reason**: {evaluation_results[\"CoherenceReason\"]}'))\n",
    "\tdisplay(Markdown(f'**Tonality Score**: {evaluation_results[\"TonalityScore\"]}'))\n",
    "\tdisplay(Markdown(f'**Tonality Reason**: {evaluation_results[\"TonalityReason\"]}'))\n",
    "\tdisplay(Markdown(f'**Safety Score**: {evaluation_results[\"SafetyScore\"]}'))\n",
    "\tdisplay(Markdown(f'**Safety Reason**: {evaluation_results[\"SafetyReason\"]}'))\n",
    "\n",
    "\n",
    "# Evaulate summary and print evaluation results\n",
    "evaluation_results = evaluate_summary(\n",
    "\t\t\t\t\t\tmodel=evalution_model,\n",
    "\t\t\t\t\t\tinput=PROMPT.format(article=article_text),\n",
    "\t\t\t\t\t\tactual_output=summary_response_output_text\n",
    "\t\t\t\t\t)\n",
    "print_evaluation_results('Article summary evaluation:', evaluation_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c000bb60",
   "metadata": {},
   "source": [
    "# Enhancement\n",
    "\n",
    "Of course, evaluation is important, but we want our system to self-correct.  \n",
    "\n",
    "+ Use the context, summary, and evaluation that you produced in the steps above to create a new prompt that enhances the summary.\n",
    "+ Evaluate the new summary using the same function.\n",
    "+ Report your results. Did you get a better output? Why? Do you think these controls are enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef6dc28",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "- I experimented with the promts and kept adjusting to improve results. Clearer and more explicit intstructions contribute to better results.   \n",
    "\n",
    "- I observed that by lowering temperature to 0.8, I got better results with improved summary generation and also improved enhancement outcomes.\n",
    "\n",
    "- I also realized that the enhancement output is not always better than the original (sometimes inconsistent) and it was inflenced by the temperature setting for the generation task. \n",
    "- For this reason the enhancement may need to be run multiple times in a loop to try to get satisfactory metrics.\n",
    "- So, I added another code cell after this one with a looped solution where the enhancement runs multiple times and tests satisfactory score has been achieved (with a loop max iteration limit).\n",
    "\n",
    "- AI as judge was able to consistently catch hallucination, but wasn't very consitent with the SummarizationMetric. This highlights the probabilistic nature of the AI judges.\n",
    "Probably using more advanced models would produce better results. I don't think that these controls are enought, I think that AI judges should be combined with the human evaluation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4cf01e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced article summary:\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Title**: Managing Oneself"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Author**: Peter F. Drucker"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Relevance**: This article is crucial for AI professionals as it highlights the importance of self-management in evolving work contexts. As AI professionals often operate in dynamic and innovative fields, understanding personal strengths, learning styles, and values is essential for adapting and contributing effectively to their organizations, thereby advancing their careers."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Summary**: In 'Managing Oneself,' Peter F. Drucker explores the necessity of self-management for individuals, particularly knowledge workers, in the contemporary economy. He emphasizes the importance of self-awareness, urging individuals to identify their strengths through feedback analysis and to comprehend their personal work styles and values. Drucker argues that true excellence and career satisfaction are achieved by capitalizing on personal strengths and aligning work with one's values. The article also discusses the need for adaptation to change, effective relationship management, and planning for the latter stages of one's career. Continuous self-assessment and adaptation are presented as vital for sustained professional success."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Tone**: Formal Academic"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**InputTokens**: 13275"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**OutputTokens**: 212"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Summarization: 100.00% pass rate\n",
      "Clarity [GEval]: 100.00% pass rate\n",
      "Professionalism [GEval]: 100.00% pass rate\n",
      "PII Leakage [GEval]: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Evaluation completed üéâ! <span style=\"font-weight: bold\">(</span>time taken: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.</span>52s | token cost: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0049242</span> USD<span style=\"font-weight: bold\">)</span>\n",
       "¬ª Test Results <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> total tests<span style=\"font-weight: bold\">)</span>:\n",
       "   ¬ª Pass Rate: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100.0</span>% | Passed: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span> | Failed: <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0</span>\n",
       "\n",
       " ================================================================================ \n",
       "\n",
       "¬ª What to share evals with your team, or a place for your test cases to live? ‚ù§Ô∏è üè°\n",
       "  ¬ª Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval view'</span> to analyze and save testing results on <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span>.\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n",
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Evaluation completed üéâ! \u001b[1m(\u001b[0mtime taken: \u001b[1;36m10.\u001b[0m52s | token cost: \u001b[1;36m0.0049242\u001b[0m USD\u001b[1m)\u001b[0m\n",
       "¬ª Test Results \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m total tests\u001b[1m)\u001b[0m:\n",
       "   ¬ª Pass Rate: \u001b[1;36m100.0\u001b[0m% | Passed: \u001b[1;32m1\u001b[0m | Failed: \u001b[1;31m0\u001b[0m\n",
       "\n",
       " ================================================================================ \n",
       "\n",
       "¬ª What to share evals with your team, or a place for your test cases to live? ‚ù§Ô∏è üè°\n",
       "  ¬ª Run \u001b[1;32m'deepeval view'\u001b[0m to analyze and save testing results on \u001b[38;2;106;0;255mConfident AI\u001b[0m.\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation after enhancement:\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Summarization Score**: 0.7272727272727273"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Summarization Reason**: The score is 0.73 because the summary introduces extra information that is not present in the original text, which may lead to misinterpretation of the original message. However, it does not contradict any key points, maintaining a reasonable level of accuracy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Coherence Score**: 0.8835483541199789"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Coherence Reason**: The response uses clear and direct language, effectively summarizing Drucker's key concepts without unnecessary jargon. It presents complex ideas about self-management and personal strengths in an accessible manner. The explanation is fluent and easy to read, although a minor improvement could be made by simplifying some phrases for even greater clarity."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Tonality Score**: 0.9974042644733071"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Tonality Reason**: The response maintains a formal academic tone throughout, reflecting expertise in the subject matter. The language used is appropriate for the context, avoiding casual expressions and ambiguity. It clearly articulates the relevance of the article to AI professionals and summarizes the key points effectively, demonstrating clarity and respectfulness in its phrasing."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Safety Score**: 0.9859363757202584"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Safety Reason**: The output does not contain any real personal information or sensitive data, adhering to all evaluation steps. It provides a summary of a published work without revealing any identifiable details about individuals. The content is appropriately anonymized and focuses on the themes of the article, ensuring user privacy is maintained."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original evaluation (for reference):\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Summarization Score**: 0.7142857142857143"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Summarization Reason**: The score is 0.71 because the summary includes extra information that was not present in the original text, which may lead to misinterpretation of the original message. However, it does not contradict any key points, maintaining a reasonable level of accuracy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Coherence Score**: 0.8867035747777032"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Coherence Reason**: The response uses clear and direct language, effectively communicating the relevance of Drucker's work to AI professionals. It avoids jargon and presents complex ideas, such as self-awareness and career development, in an accessible manner. The summary is well-structured and easy to follow, although a slight improvement could be made in reducing any potential vagueness in the advice given. Overall, the fluency and readability are strong."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Tonality Score**: 0.9979667648735051"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Tonality Reason**: The response maintains a formal academic tone throughout, reflecting expertise in the subject matter. The language used is appropriate for an academic audience, avoiding casual expressions and ambiguity. The content is contextually relevant to AI professionals, emphasizing self-management and adaptability, which aligns well with the evaluation criteria. Additionally, the summary is clear, respectful, and free from slang, demonstrating a strong alignment with the evaluation steps."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Safety Score**: 0.9875683488723995"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Safety Reason**: The output does not contain any real personal information or sensitive data, adhering to all evaluation steps. It focuses on the content of the book and its relevance to AI professionals without exposing any identifiable information. The use of the author's name is appropriate as it is publicly available and does not compromise privacy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ENHANCEMENT_SYSTEM_PROMPT = \"\"\"\n",
    "\tYou are a senior academic researcher that reads and improves article summaries written by your colleagues using Formal Academic tone. \n",
    "\tYour read article and the summary and also summary evaluation provided by an independent critic.\n",
    "\tThen you write a new and improved summary considering critic's evalution.\n",
    "\t\"\"\"\n",
    "\n",
    "ENHANCEMENT_PROMPT = \"\"\"\n",
    "\n",
    "\tGiven the following context from <article>, <summary> and <summary_evaluation> do the following:\n",
    "\n",
    "\t1.\tIdentify the articles's title and author.\n",
    "\n",
    "\t2.\tExplain relevance of this article for an AI professonal in their professional development. This should be a statement, no longer than one paragraph.\t\n",
    "\t\tDo not add any extra information that is not present in the original text.\n",
    "\n",
    "\t3.\tRead and consider feedback in <summary_evaluation> and then, summarize provided article concisely and succinct with no more than 1000 tokens.\t\t\n",
    "\t\tDo not add any extra information that is not present in the original text.\n",
    "\n",
    "\t\t<summary_evaluation> contains score and reason for each metric as follows: \n",
    "\n",
    "\t\t\ta) 'Summarization' has: 'SummarizationScore': score and 'SummarizationReason': reason,\n",
    "\t\t\tb) 'Coherence' has 'CoherenceScore': score, 'CoherenceReason': reason,\n",
    "\t\t\tc) 'Tonality' => 'TonalityScore': score, 'TonalityReason': reason,\n",
    "\t\t\td) 'Safety' => 'SafetyScore': score, 'SafetyReason': reason,\n",
    "\t\tScore value is a float number between 0 (worst) and 1 (best). Read 'score' value and improve summary as per feedback in the 'reason' field. \t\t\n",
    "\n",
    "\t4. Provide your response as structured output as per provided schema. Do not fill in optional fields.\n",
    "\n",
    "\tThe aricle is the following: \n",
    "\t<article>\n",
    "\t{article}\n",
    "\t</article>\n",
    "\n",
    "\tThe summary written by your colleague is the following:\n",
    "\t<summary>\n",
    "\t{summary}\n",
    "\t</summary>\n",
    "\n",
    "\tThe summary evaluation provided by the critic is the following:\n",
    "\t<summary_evaluation>\n",
    "\t{evaluation}\n",
    "\t</summary_evaluation>\n",
    "\"\"\"\t\n",
    "\n",
    "# Generate and print enhanced summary\n",
    "enhanced_article_summary, enhanced_summary_response_output_text = generate_article_summary(\t\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tENHANCEMENT_SYSTEM_PROMPT, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tENHANCEMENT_PROMPT.format(\t\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tarticle=article_text,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsummary=summary_response_output_text,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tevaluation=evaluation_results)\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t)\n",
    "print_article_summary('Enhanced article summary:', enhanced_article_summary)\n",
    "\n",
    "# Evaluate enhanced summary\n",
    "evaluation_result_of_enhanced_summary = evaluate_summary(\t\n",
    "\t\t\t\t\t\t\t\t\t\t\tmodel=evalution_model, \n",
    "\t\t\t\t\t\t\t\t\t\t\tinput=PROMPT.format(article=article_text), \n",
    "\t\t\t\t\t\t\t\t\t\t\tactual_output=enhanced_summary_response_output_text\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\n",
    "# Print evaluations (enhanced and original)\n",
    "print_evaluation_results('Evaluation after enhancement:', evaluation_result_of_enhanced_summary)\n",
    "print_evaluation_results('\\nOriginal evaluation (for reference):', evaluation_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc54ddc1",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "- Below I also created a looped solution where the enhancement can be run multiple times to try to achieve desired satisfactory score (with a loop max iteration limit) \n",
    "- It checks the worst score among tested metrics with min(score1, score2, ...) function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53867b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original article summary:\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Title**: Managing Oneself"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Author**: Peter F. Drucker"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Relevance**: This article is highly relevant for AI professionals as it emphasizes the importance of self-management in a knowledge-driven economy. By understanding personal strengths, work styles, and values, AI professionals can effectively navigate their careers, make significant contributions, and achieve excellence. This self-awareness is crucial for adapting to the rapidly changing technological landscape and maintaining long-term career growth in the AI field."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Summary**: In \"Managing Oneself,\" Peter F. Drucker discusses the necessity for individuals to be their own chief executive officers in the modern knowledge economy. He argues that success depends on a deep understanding of one's strengths, weaknesses, values, and work style. Drucker introduces feedback analysis as a tool to identify strengths and areas for improvement. He emphasizes the importance of aligning personal values with organizational values to achieve satisfaction and performance. The article also explores how knowing one's strengths and preferred work environment can lead to more effective career decisions. Additionally, Drucker highlights the need to manage personal relationships and communication in the workplace, as well as the importance of planning for a second career or parallel career to maintain engagement and productivity throughout a lifetime. Overall, self-management is presented as a continuous process crucial for personal and professional success."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Tone**: Formal Academic"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**InputTokens**: 12436"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**OutputTokens**: 270"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Summarization: 100.00% pass rate\n",
      "Clarity [GEval]: 100.00% pass rate\n",
      "Professionalism [GEval]: 100.00% pass rate\n",
      "PII Leakage [GEval]: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Evaluation completed üéâ! <span style=\"font-weight: bold\">(</span>time taken: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.</span>94s | token cost: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0048978</span> USD<span style=\"font-weight: bold\">)</span>\n",
       "¬ª Test Results <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> total tests<span style=\"font-weight: bold\">)</span>:\n",
       "   ¬ª Pass Rate: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100.0</span>% | Passed: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span> | Failed: <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0</span>\n",
       "\n",
       " ================================================================================ \n",
       "\n",
       "¬ª What to share evals with your team, or a place for your test cases to live? ‚ù§Ô∏è üè°\n",
       "  ¬ª Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval view'</span> to analyze and save testing results on <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span>.\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n",
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Evaluation completed üéâ! \u001b[1m(\u001b[0mtime taken: \u001b[1;36m7.\u001b[0m94s | token cost: \u001b[1;36m0.0048978\u001b[0m USD\u001b[1m)\u001b[0m\n",
       "¬ª Test Results \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m total tests\u001b[1m)\u001b[0m:\n",
       "   ¬ª Pass Rate: \u001b[1;36m100.0\u001b[0m% | Passed: \u001b[1;32m1\u001b[0m | Failed: \u001b[1;31m0\u001b[0m\n",
       "\n",
       " ================================================================================ \n",
       "\n",
       "¬ª What to share evals with your team, or a place for your test cases to live? ‚ù§Ô∏è üè°\n",
       "  ¬ª Run \u001b[1;32m'deepeval view'\u001b[0m to analyze and save testing results on \u001b[38;2;106;0;255mConfident AI\u001b[0m.\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original article summary evaluation:\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Summarization Score**: 1.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Summarization Reason**: The score is 1.00 because the summary accurately reflects the original text without any contradictions or extra information, demonstrating a perfect alignment with the source material."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Coherence Score**: 0.8924141814233308"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Coherence Reason**: The response uses clear and direct language, effectively communicating the relevance of Drucker's work to AI professionals. It avoids jargon and presents complex ideas, such as self-management and feedback analysis, in an accessible manner. The summary is well-structured and easy to follow, with no vague or confusing parts. Overall, the fluency and readability are strong, making it a highly effective response."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Tonality Score**: 0.9946596667375396"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Tonality Reason**: The response maintains a formal academic tone throughout, reflecting expertise in the subject matter. The language is appropriate for the context, avoiding casual expressions and ensuring clarity. The summary effectively conveys the key concepts of self-management as discussed by Drucker, aligning well with the evaluation steps."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Safety Score**: 0.993133263140737"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Safety Reason**: The output does not include any real personal information or sensitive data, adhering to all evaluation steps. It provides a summary of a published work without revealing any identifiable details about individuals. The content is appropriately anonymized and focuses on the themes of self-management relevant to AI professionals, ensuring user privacy is maintained."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No need to run enhancemnet since satisfactory scores ( >= 0.8) were already achieved. \n",
      "----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------------------------------\n",
    "# Below we run enhancement multiple times to try to achieve desired score (with max iteration limit) \n",
    "# (uses functions defined above)\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "def get_worst_score(evaluation_results: dict) -> float:\n",
    "\treturn min(evaluation_results[\"SummarizationScore\"], evaluation_results[\"CoherenceScore\"], evaluation_results[\"TonalityScore\"], evaluation_results[\"SafetyScore\"])\n",
    "\n",
    "\n",
    "SATISFACTORY_SCORE = 0.8\n",
    "MAX_ENHANCEMENTS_TRIES_LIMIT = 3\n",
    "\n",
    "# First run (generation and evaluation):\n",
    "article_summary, summary_response_output_text = generate_article_summary(SYSTEM_PROMPT, PROMPT.format(article=article_text))\n",
    "print_article_summary('Original article summary:', article_summary)\n",
    "evaluation_results = evaluate_summary(model=evalution_model, input=PROMPT.format(article=article_text), actual_output=summary_response_output_text)\n",
    "evaluation_worst_score = get_worst_score(evaluation_results)\n",
    "print_evaluation_results('Original article summary evaluation:', evaluation_results)\n",
    "\n",
    "# Enhancements iteration (if needed):\n",
    "if evaluation_worst_score >= SATISFACTORY_SCORE:\n",
    "\tprint(f'\\nNo need to run enhancemnet since satisfactory scores ( >= {SATISFACTORY_SCORE}) were already achieved. \\n----------------------------------------------------------------------------')\n",
    "else:\t\n",
    "\tprint('\\n\\nTrying to enhance/improve results:\\n--------------------------------------------------')\n",
    "\tfor i in range(MAX_ENHANCEMENTS_TRIES_LIMIT):\n",
    "\t\tprint(f'Enhancement iteration: {i+1}\\n')\n",
    "\t\tarticle_summary, summary_response_output_text = generate_article_summary(\n",
    "\t\t\t\t\t\t\t\t\t\tENHANCEMENT_SYSTEM_PROMPT, \n",
    "\t\t\t\t\t\t\t\t\t\tENHANCEMENT_PROMPT.format(\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tarticle=article_text,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tsummary=summary_response_output_text,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tevaluation=evaluation_results)\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tevaluation_results = evaluate_summary(\n",
    "\t\t\t\t\t\t\t\t\t\tmodel=evalution_model, \n",
    "\t\t\t\t\t\t\t\t\t\tinput=PROMPT.format(article=article_text),\n",
    "\t\t\t\t\t\t\t\t\t\tactual_output=summary_response_output_text\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\tevaluation_worst_score = get_worst_score(evaluation_results)\n",
    "\t\tif evaluation_worst_score >= SATISFACTORY_SCORE:\n",
    "\t\t\tbreak\n",
    "\t\telse:\n",
    "\t\t\tprint_evaluation_results('\\nIteration article summary evalutation:', evaluation_results)\n",
    "\n",
    "\tif evaluation_worst_score < SATISFACTORY_SCORE:\n",
    "\t\tprint(f'\\nCould not achieve satisfactory scores of {SATISFACTORY_SCORE} with {MAX_ENHANCEMENTS_TRIES_LIMIT} iterations.')\n",
    "\n",
    "\tprint_article_summary('\\nFinal article summary:', article_summary)\n",
    "\tprint_evaluation_results('\\nFinal article summary evalutation:', evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d0de25",
   "metadata": {},
   "source": [
    "Please, do not forget to add your comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e81f47",
   "metadata": {},
   "source": [
    "\n",
    "# Submission Information\n",
    "\n",
    "üö® **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** üö® for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "## Submission Parameters\n",
    "\n",
    "- The Submission Due Date is indicated in the [readme](../README.md#schedule) file.\n",
    "- The branch name for your repo should be: assignment-1\n",
    "- What to submit for this assignment:\n",
    "    + This Jupyter Notebook (assignment_1.ipynb) should be populated and should be the only change in your pull request.\n",
    "- What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    + Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "## Checklist\n",
    "\n",
    "+ Created a branch with the correct naming convention.\n",
    "+ Ensured that the repository is public.\n",
    "+ Reviewed the PR description guidelines and adhered to them.\n",
    "+ Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deploying-ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
